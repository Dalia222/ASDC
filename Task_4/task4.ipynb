{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_X6cHBBibRnj"
      },
      "outputs": [],
      "source": [
        "#@title imports and installing\n",
        "!pip install torch torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install torch==1.10.0+cu102 torchvision==0.11.1+cu102 torchaudio==0.10.0+cu102 -f https://download.pytorch.org/whl/cu102/torch_stable.html\n",
        "\n",
        "# Install Detectron2\n",
        "!pip install git+https://github.com/facebookresearch/detectron2.git\n"
      ],
      "metadata": {
        "id": "BOJmLYflx5Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch torchvision\n",
        "!pip install git+https://github.com/facebookresearch/fvcore.git\n",
        "!pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html\n"
      ],
      "metadata": {
        "id": "FO16_FInwB0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.10.0+cu102 torchvision==0.11.1+cu102 torchaudio==0.10.0+cu102 -f https://download.pytorch.org/whl/cu102/torch_stable.html\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.10/index.html\n"
      ],
      "metadata": {
        "id": "ooMMbdz8xDcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# with open(\"/content/drive/MyDrive/Task_4/train/_annotations.coco.json\") as f:\n",
        "#     json.load(f)\n",
        "\n",
        "# with open(\"/content/drive/MyDrive/Task_4/valid/_annotations.coco.json\") as f:\n",
        "#     json.load(f)\n",
        "import string\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Task_4/train/_annotations.coco.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "    content = ''.join(filter(lambda x: x in string.printable, content))\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Task_4/train/_annotations_cleaned.coco.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(content)\n"
      ],
      "metadata": {
        "id": "_kZvygfW7d6f"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
        "from detectron2 import model_zoo\n",
        "import os\n",
        "import cv2\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "import random\n",
        "import json\n",
        "import string\n",
        "\n",
        "def clean_json(json_path, cleaned_json_path):\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "            cleaned_data = json.dumps(data, ensure_ascii=False, separators=(',', ':'))\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON in {json_path}: {e}\")\n",
        "            return\n",
        "\n",
        "    with open(cleaned_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(cleaned_data)\n",
        "\n",
        "# Path to the original JSON files\n",
        "train_json_path = \"/content/drive/MyDrive/Task_4/train/_annotations.coco.json\"\n",
        "val_json_path = \"/content/drive/MyDrive/Task_4/valid/_annotations.coco.json\"\n",
        "\n",
        "# Clean the training JSON file\n",
        "cleaned_train_json_path = \"/content/drive/MyDrive/Task_4/train/_annotations_cleaned.coco.json\"\n",
        "clean_json(train_json_path, cleaned_train_json_path)\n",
        "\n",
        "# Clean the validation JSON file\n",
        "cleaned_val_json_path = \"/content/drive/MyDrive/Task_4/valid/_annotations_cleaned.coco.json\"\n",
        "clean_json(val_json_path, cleaned_val_json_path)\n",
        "\n",
        "# Unregister the 'dentistry_dataset_train' dataset if it's already registered\n",
        "if \"dentistry_dataset_train_v2\" in DatasetCatalog:\n",
        "    DatasetCatalog.remove(\"dentistry_dataset_train_v2\")\n",
        "    MetadataCatalog.remove(\"dentistry_dataset_train_v2\")\n",
        "\n",
        "# Unregister the 'dentistry_dataset_val' dataset if it's already registered\n",
        "if \"dentistry_dataset_val_v2\" in DatasetCatalog:\n",
        "    DatasetCatalog.remove(\"dentistry_dataset_val_v2\")\n",
        "    MetadataCatalog.remove(\"dentistry_dataset_val_v2\")\n",
        "\n",
        "# Register the Roboflow dataset with the cleaned JSON files\n",
        "register_coco_instances(\"dentistry_dataset_train_v2\", {}, cleaned_train_json_path, \"/content/drive/MyDrive/Task_4/train\")\n",
        "register_coco_instances(\"dentistry_dataset_val_v2\", {}, cleaned_val_json_path, \"/content/drive/MyDrive/Task_4/valid\")\n",
        "\n",
        "# Configuration and training\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"dentistry_dataset_train_v2\",)\n",
        "cfg.DATASETS.TEST = (\"dentistry_dataset_val_v2\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 300\n",
        "cfg.SOLVER.STEPS = []  # Adjust this based on your dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # Number of classes (including background)\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "# Perform inference and visualization\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Adjust as needed\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Visualization\n",
        "dataset_dicts = DatasetCatalog.get(\"dentistry_dataset_val_v2\")\n",
        "for d in random.sample(dataset_dicts, 3):  # Change 3 to the number of images you want to visualize\n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(\"dentistry_dataset_val_v2\"), scale=1.2)\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2.imshow(\"Visualization\", v.get_image()[:, :, ::-1])\n",
        "    cv2.waitKey(0)\n"
      ],
      "metadata": {
        "id": "D-uGGX1l2i_F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}